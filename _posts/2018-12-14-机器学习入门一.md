---
title: 机器学习入门（一）
description: 机器学习
categories:
 - AI
 - Machine Learning
tags:
---

> 机器学习



## 极简主义的七个要点：## 一、学习分类


![image.png | center | 672x253](https://cdn.nlark.com/lark/0/2018/png/18245/1544081733543-eb14fd1d-a5b6-4cd1-8840-51cfae6d6c28.png "")

### 1.监督学习


![image.png | center | 429x342](https://cdn.nlark.com/lark/0/2018/png/18245/1543489015254-4a5a759a-0b85-4828-a59b-cf5f8884f968.png "")

   <span data-type="color" style="color:rgb(0, 0, 0)"><span data-type="background" style="background-color:rgb(255, 255, 255)">监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果。</span></span>
### 2.非监督学习


![image.png | center | 300x225](https://cdn.nlark.com/lark/0/2018/png/18245/1543489084725-a282fa01-2d0a-4bdc-959c-67dc03e48049.png "")

    <span data-type="color" style="color:rgb(0, 0, 0)"><span data-type="background" style="background-color:rgb(255, 255, 255)">非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。</span></span>
### 3.半监督学习


![image.png | center | 300x200](https://cdn.nlark.com/lark/0/2018/png/18245/1543489146979-d42c5208-e505-470a-b49a-57bc77769acc.png "")

   <span data-type="color" style="color:rgb(0, 0, 0)"><span data-type="background" style="background-color:rgb(255, 255, 255)">输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。</span></span>
### 4.强化学习


![image.png | center | 300x176](https://cdn.nlark.com/lark/0/2018/png/18245/1543489215987-488aec78-5658-42f6-b637-951517c0ff85.png "")

     输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。 
　　在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。 而强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。

## 二、算法学习
### 1.朴素贝叶斯
贝叶斯定律
```plain
P(A|B) = P(B|A) P(A) / P(B)
```


![image.png | center | 325x212](https://cdn.nlark.com/lark/0/2018/png/18245/1544082203000-7273fedc-be43-4bfc-8fc5-2071d11254fe.png "")

<span data-type="color" style="color:black">   </span>假设某个体<span data-type="color" style="color:black">有n项特征（Feature），分别为F1、F2、...、Fn。现有m个类别（Category），分别为C1、C2、...、Cm。</span><span data-type="color" style="color:red">贝叶斯分类器</span><span data-type="color" style="color:black">就是计算出概率最大的那个分类，也就是求下面这个算式的最大值：</span>
```plain
P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)
```
<span data-type="color" style="color:black">     由于P(F1F2...Fn)对于所有的类别都是相同的，可以省略，问题就变成了求</span>
```plain
Max[P(F1F2...Fn|C)P(C)]
```
<span data-type="color" style="color:black">     </span>朴素贝叶斯分类器则是更进一步<span data-type="color" style="color:black">，假设所有特征都彼此独立，因此</span>
```plain
P(F1F2...Fn|C)P(C) = P(F1|C)P(F2|C) ... P(Fn|C)P(C)
```
<span data-type="color" style="color:black">上式等号右边的每一项，都可以从统计资料中得到，由此就可以计算出每个类别对应的概率，从而找出最大概率的那个类。</span>
举例：



### 2.决策树
CART

### 3.线性回归


![image.png | center | 747x569](https://cdn.nlark.com/lark/0/2018/png/18245/1544087241432-b0ed2a72-009c-41ef-8ad1-6430b8754c92.png "")

<span data-type="color" style="color:rgb(47, 47, 47)"><span data-type="background" style="background-color:rgb(255, 255, 255)">线性回归是在假设特征满足线性关系，根据给定的训练数据训练一个模型，并用此模型进行预测。其函数形式为：</span></span>


![image.png | center | 184x52](https://cdn.nlark.com/lark/0/2018/png/18245/1544087827856-371f7306-5b94-4c94-a246-ffd2660a8ede.png "")

损失函数：


![image.png | center | 370x62](https://cdn.nlark.com/lark/0/2018/png/18245/1544087593021-66f13caf-f50e-435e-8607-f8512aa5bb79.png "")

如何使得损失函数最小？<span data-type="color" style="color:rgb(47, 47, 47)"><span data-type="background" style="background-color:rgb(255, 255, 255)">可以对这个函数进行求导：</span></span>


![image.png | center | 440x88](https://cdn.nlark.com/lark/0/2018/png/18245/1544087621126-736087f7-8c0b-42c4-8266-ece0b66e5563.png "")




![image.png | center | 689x85](https://cdn.nlark.com/lark/0/2018/png/18245/1544087649286-860efc60-8d64-4b11-af77-f38ed142202d.png "")

<span data-type="color" style="color:rgb(47, 47, 47)"><span data-type="background" style="background-color:rgb(255, 255, 255)">令这两个偏导等于0，求解方程组，解得：</span></span>


![image.png | center | 497x185](https://cdn.nlark.com/lark/0/2018/png/18245/1544087691288-e9d4f1f8-e16e-4749-b624-55b4d738618d.png "")



![image.png | center | 424x88](https://cdn.nlark.com/lark/0/2018/png/18245/1544087707905-46ceefc1-b155-41be-9014-8253018dc26c.png "")

<span data-type="color" style="color:rgb(47, 47, 47)"><span data-type="background" style="background-color:rgb(255, 255, 255)">就是模型的最优参数，我们将参数代入模型中，便可以进行预测。</span></span>
对于多维：


![image.png | center | 204x68](https://cdn.nlark.com/lark/0/2018/png/18245/1544087961625-eb253dc8-c76e-48e8-88e8-748142e1ee50.png "")

损失函数：


![image.png | center | 231x55](https://cdn.nlark.com/lark/0/2018/png/18245/1544087989701-cd1bee98-44e3-408c-84d7-f5a10960c203.png "")

梯度下降
<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">首先,为了简化问题，我们假设只有一组样本,即m=1，</span></span>求偏导：


![image.png | center | 404x238](https://cdn.nlark.com/lark/0/2018/png/18245/1544090616142-60c04b84-7881-43b8-8381-86dcc25f055a.png "")

<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">其梯度下降的迭代公式可表示为：</span></span>


![image.png | center | 223x40](https://cdn.nlark.com/lark/0/2018/png/18245/1544090692872-f6098214-0bc8-4e7e-b3ce-4dbae82f20ae.png "")


其中，a表示学习率，这里做一个不严谨的类比，假设你在爬山，这会儿正在下山的图中，那么你朝着哪个方向下降速度会最快呢？答案显然是，梯度方向，a就是你要迈的步子长度。注意：通常a是手动设置的，若a值设的太小，会导致步子迈的太小，你将花较长的时间下山，也就是函数要花很长的时间才能收敛；若a值设的过大，你的算法可能会迈过最小值，因为你的步子太大了
。
推广到n维：


![image.png | center | 455x65](https://cdn.nlark.com/lark/0/2018/png/18245/1544090809671-824dde7a-704f-476b-922c-a96e1327b647.png "")

__批梯度下降（batch gradient descent）__
这种新的表达式每一步都是计算的全部训练集的数据，所以称之为__批梯度下降（batch gradient descent）__。
    注意，梯度下降可能得到局部最优，但在优化问题里我们已经证明线性回归只有一个最优点，因为损失函数J(θ)是一个二次的凸函数，不会产生局部最优的情况。（假设学习步长α不是特别大）
__随机梯度下降（Stochastic Gradient Descent, SGD）__
    随机梯度下降在计算下降最快的方向时时随机选一个数据进行计算，而不是扫描全部训练数据集，这样就加快了迭代速度。

### 4.逻辑回归
<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">如果我们想知道对于一个二类分类问题，对于具体的一个样例，我们不仅想知道该类属于某一类，而且还想知道该类属于某一类的概率多大,有什么办法呢？</span></span>
<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">y</span></span>=wx+b
y的阈值处于（−∞，+∞），此时不能很好的给出属于某一类的概率，因为概率的范围是[0,1],我们需要一个更好的映射函数，能够将分类的结果很好的映射成为[0,1]之间的概率，并且这个函数能够具有很好的可微分性。在这种需求下，人们找到了这个映射函数，即逻辑斯谛函数，也就是我们常说的sigmoid函数，其形式如下


![image.png | center | 236x119](https://cdn.nlark.com/lark/0/2018/png/18245/1544089091758-5e8b57dd-c868-41ee-b8ed-46592f4b3d1c.png "")



![image.png | center | 747x557](https://cdn.nlark.com/lark/0/2018/png/18245/1544089075306-90387a76-a042-4699-bee1-8a25ec8076c8.png "")


* sigmoid函数完美的解决了上述需求，而且sigmoid函数连续可微分。
* 假设数据离散二类可分，分为0类和1类,如果概率值大于1/2，我们就将该类划分为1类，如果概率值低于1/2,我们就将该类划分为0类。当z取值为0的时候，概率值为1/2，这时候需要人为规定划分为哪一类

<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">逻辑斯蒂回归是针对线性可分问题的一种易于实现而且性能优异的分类模型，是使用最为广泛的分类模型之一。假设某件事发生的概率为p，那么这件事不发生的概率为(1-p)，我们称p/(1-p)为这件事情发生的几率。取这件事情发生几率的对数，定义为logit(p)，所以logit(p)为</span></span>


![image.png | center | 205x77](https://cdn.nlark.com/lark/0/2018/png/18245/1544089488643-8ea67d8a-a402-4a30-aabd-b465c98b0924.png "")

因为logit函数的输入取值范围为[0,1](%E5%9B%A0%E4%B8%BAp%E4%B8%BA%E6%9F%90%E4%BB%B6%E4%BA%8B%E6%83%85%E5%8F%91%E7%94%9F%E7%9A%84%E6%A6%82%E7%8E%87)，所以通过logit函数可以将输入区间为[0,1]转换到整个实数范围内的输出,log函数图像如下


![image.png | center | 426x426](https://cdn.nlark.com/lark/0/2018/png/18245/1544089517512-c8f48140-9252-42cd-9a72-fb2c2eb92f55.png "")

损失函数：


![image.png | center | 747x77](https://cdn.nlark.com/lark/0/2018/png/18245/1544090170472-5a7fabcd-e761-44d2-bccc-fb9ac6250fed.png "")



![image.png | center | 441x297](https://cdn.nlark.com/lark/0/2018/png/18245/1544090205825-025283de-6057-4365-9220-dd60a875a91c.png "")

在这种情况下，我们φ(z(i))表示sigmoid对第i个值的预测结果，我们将sigmoid函数带入上述成本函数中，绘制其图像，发现这个成本函数的函数图像是一个非凸函数，如下图所示，这个函数里面有很多极小值，如果采用梯度下降法，则会导致陷入局部最优解中,有没有一个凸函数的成本函数呢
？
<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">假设sigmoid函数φ(z)表示属于1类的概率，于是做出如下的定义：</span></span>


![image.png | center | 396x96](https://cdn.nlark.com/lark/0/2018/png/18245/1544090273497-b2419266-4d48-4a60-b796-859c496aff5f.png "")

<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">将两个式子综合来，可以改写为下式：</span></span>


![image.png | center | 306x44](https://cdn.nlark.com/lark/0/2018/png/18245/1544090301121-57227ab2-73d6-432e-b08f-355dd310a927.png "")

<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">上式将分类为0和分类和1的概率计算公式合二为一。假设分类器分类足够准确，此时对于一个样本，如果它是属于1类，分类器求出的属于1类的概率应该尽可能大，即p(y=1lx)尽可能接近1；如果它是0类，分类器求出的属于0类的概率应该尽可能大，即p(y=0lx)尽可能接近1。</span></span>
<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">通过上述公式对二类分类的情况分析，可知我们的目的是求取参数w和b，使得p(ylx)对0类和1类的分类结果尽可能取最大值，然而实际上我们定义的损失函数的是求最小值，于是，很自然的，我们想到对p(ylx)式子加一个负号，就变成了求最小值的问题，这就得到了逻辑回归中的损失函数。</span></span>
<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">不过，为了计算方便，我们通常对上述式子取log，因而得到下式：</span></span>


![image.png | center | 557x139](https://cdn.nlark.com/lark/0/2018/png/18245/1544090355142-54abd98c-6c13-4201-ae87-21f8cce8bc8d.png "")

* 公式(1)是对概率公式取log，公式(2)是对公式(1)取相反数。上述公式的函数图像如下图所示。这是一个凸函数（斜率是非单调递减的函数即凸函数），因此可以用梯度下降法求其最小值。


![image.png | center | 632x408](https://cdn.nlark.com/lark/0/2018/png/18245/1544090383024-18a9c4df-27cd-4eb5-a3f6-9d8f78519ae2.png "")


* 根据损失函数是单个样本的预测值和实际值的误差，而成本函数是全部样本的预测值和实际值之间的误差，于是对所有样本的损失值取平均数，得到我们的成本函数：


![image.png | center | 401x101](https://cdn.nlark.com/lark/0/2018/png/18245/1544090429402-11ee6b4b-4a50-40d6-af2b-0e1b1b17f1ab.png "")

<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">损失函数是凸函数，m个损失函数的和仍然是凸函数，因而可以用梯度下降法求最小值</span></span>

### 5.KNN


![image.png | center | 342x316](https://cdn.nlark.com/lark/0/2018/png/18245/1544605713698-f2a1a587-ee14-4558-9301-c90f1a8e5480.png "")

KNN是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。
     通过一个简单的例子说明一下：如上图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。
由此也说明了KNN算法的结果很大程度取决于K的选择。
     在KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离：


![image.png | left | 747x86](https://cdn.nlark.com/lark/0/2018/png/18245/1544605787691-216dc1b1-2d03-45c6-8077-dda0828ceb7b.png "")

同时，KNN通过依据k个对象中占优的类别进行决策，而不是单一的对象类别决策。这两点就是KNN算法的优势。
   接下来对KNN算法的思想总结一下：就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：
1）计算测试数据与各个训练数据之间的距离；
2）按照距离的递增关系进行排序；
3）选取距离最小的K个点；
4）确定前K个点所在类别的出现频率；
5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。
### 6.K-Means
k-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。 
  k-means算法中的k代表类簇个数，means代表类簇内数据对象的均值（这种均值是一种对类簇中心的描述），因此，k-means算法又称为k-均值算法。k-means算法是一种基于划分的聚类算法，以距离作为数据对象间相似性度量的标准，即数据对象间的距离越小，则它们的相似性越高，则它们越有可能在同一个类簇。数据对象间距离的计算有很多种，k-means算法通常采用欧氏距离来计算数据对象间的距离。
<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">k-means算法以距离作为数据对象间相似性度量的标准，通常采用</span></span>__欧氏距离__<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">来计算数据对象间的距离</span></span>：


![image.png | center | 586x168](https://cdn.nlark.com/lark/0/2018/png/18245/1544606495703-c5747c26-edfd-4ab8-9ee9-c246194f4a3f.png "")

<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">k-means算法聚类过程中，每次迭代，对应的类簇中心需要重新计算（更新）：对应类簇中所有数据对象的均值，即为更新后该类簇的类簇中心。定义第k</span></span>k<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">个类簇的类簇中心为C</span></span>enterkCenterk<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">，则类簇中心更新方式如下</span></span>：


![image.png | center | 448x128](https://cdn.nlark.com/lark/0/2018/png/18245/1544606543282-2f5c499b-d59a-44f5-8bdb-6e4745a667bd.png "")


迭代终止条件：
<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">一般情况，有两种方法来终止迭代：一种方法是设定迭代次数T</span></span>T<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">，当到达第T</span></span>T<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">次迭代，则终止迭代，此时所得类簇即为最终聚类结果；另一种方法是采用误差平方和准则函数，函数模型如下： </span></span>


![image.png | center | 538x122](https://cdn.nlark.com/lark/0/2018/png/18245/1544606659135-6d8af30a-e804-4536-bf43-61f0c096efe1.png "")

<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">其中，K</span></span>K<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">表示类簇个数。当两次迭代J</span></span>J<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">的差值小于某一阈值时，即Δ</span></span>J<δΔJ<δ<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">时，则终止迭代，此时所得类簇即为最终聚类结果。</span></span>
k-means算法思想可描述为：首先初始化KK个类簇中心；然后计算各个数据对象到聚类中心的距离，把数据对象划分至距离其最近的聚类中心所在类簇中；接着根据所得类簇，更新类簇中心；然后继续计算各个数据对象到聚类中心的距离，把数据对象划分至距离其最近的聚类中心所在类簇中；接着根据所得类簇，继续更新类簇中心；……一直迭代，直到达到最大迭代次数TT，或者两次迭代JJ的差值小于某一阈值时，迭代终止，得到最终聚类结果。算法详细流程描述如下


![image.png | center | 747x435](https://cdn.nlark.com/lark/0/2018/png/18245/1544606731403-2d8b8fcb-b04d-4599-8b43-b6f3e147e3c9.png "")



![image.png | center | 701x233](https://cdn.nlark.com/lark/0/2018/png/18245/1544606754124-535873cc-8bac-429d-a9ca-b3ce0c7e2683.png "")

<span data-type="color" style="color:rgba(0, 0, 0, 0.75)"><span data-type="background" style="background-color:rgb(255, 255, 255)">其中，黑色圆点代表类簇中心，白色圆点代表待聚类数据对象。</span></span>
* 优点：
    算法简单易实现；
* 缺点：
    需要用户事先指定类簇个数K；
    聚类结果对初始类簇中心的选取较为敏感；<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">初始类簇中心的选取，可以通过</span></span>[k-means++算法](http://www.cnblogs.com/nocml/p/5150756.html)<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">进行改进</span></span>
    容易陷入局部最优；
    只能发现球型类簇；
K-means++算法：

### 9.Boosting
GBDT
adaBoost

### 10.支持向量机SVM
<span data-type="color" style="color:rgb(79, 79, 79)"><span data-type="background" style="background-color:rgb(255, 255, 255)">Svm（support Vector Mac）又称为支持向量机，是一种二分类的模型。当然如果进行修改之后也是可以用于多类别问题的分类。支持向量机可以分为线性核非线性两大类。其主要思想为找到空间中的一个更够将所有数据样本划开的超平面，并且使得本本集中所有数据到这个超平面的距离最短。</span></span>


![image.png | left | 568x267](https://cdn.nlark.com/lark/0/2018/png/18245/1544605269407-8344576a-1eab-4f22-a797-f07f9a7dab5d.png "")

推导：
法向量
空间距离
拉格朗日？
### 11.深度学习
深度学习与逻辑回归的关系[https://blog.csdn.net/tina\_ttl/article/details/51547428](https://blog.csdn.net/tina_ttl/article/details/51547428)
BP反向传播 [https://blog.csdn.net/qq\_32865355/article/details/80260212](https://blog.csdn.net/qq_32865355/article/details/80260212)
CNN 卷积神经网络
RNN 循环(递归)神经网络
GAN 生成对抗神经网络


## 三、模型评估方法
### 1、混淆矩阵


![image.png | center | 525x233](https://cdn.nlark.com/lark/0/2018/png/18245/1544080877328-25640271-3a9e-4801-984f-6125ceb37dda.png "")

__TP（实际为正预测为正）__
__FP（实际为负但预测为正）__
__TN（实际为负预测为负）__
__FN（实际为正但预测为负）__
<span data-type="color" style="color:rgb(47, 47, 47)"><span data-type="background" style="background-color:rgb(255, 255, 255)">召回率（Recall,TNR）：预测对的正例数占真正的正例数的比率计算公式</span></span>
```plain
Recall=TP / (TP+FN)
```
<span data-type="color" style="color:rgb(47, 47, 47)"><span data-type="background" style="background-color:rgb(255, 255, 255)">准确率：反映分类器统对整个样本的判定能力，能将正的判定为正，负的判定为负</span></span>
```plain
Accuracy=(TP+TN) / (TP+FP+TN+FN)
```
<span data-type="color" style="color:rgb(47, 47, 47)"><span data-type="background" style="background-color:rgb(255, 255, 255)">精准率：指的是所得数值与真实值之间的精确程度；预测正确的正例数占预测为正例总量的比率，计算公式</span></span>
```plain
Precision=TP / (TP+FP)
```
### 2.ROC图和AUC
TPR：在所有实际为阳性的样本中，被正确地判断为阳性之比率。TPR=TP/(TP+FN)
FPR：在所有实际为阴性的样本中，被错误地判断为阳性之比率。FPR=FP/(FP+TN)
<span data-type="color" style="color:rgb(0, 0, 0)"><span data-type="background" style="background-color:rgb(254, 254, 242)">以FPR为横轴，TPR为纵轴，得到如下ROC空间：</span></span>


![788753-20161121105420346-41033633.png | center | 327x326](https://cdn.nlark.com/lark/0/2018/png/18245/1544081165764-aa3ab28b-a4f7-4784-9a9d-36e96c0d30d8.png "")

左上角的点（TPR=1，FPR=0），为完美分类，也就是这个医生医术高明，诊断全对；点A（TPR>FPR），医生A的判断大体是正确的。中线上的点B（TPR=FPR），也就是医生B全都是蒙的，蒙对一半，蒙错一半；下半平面的点C（TPR<FPR），这个医生说你有病，那么你很可能没有病，医生C的话我们要反着听，为真庸医。
上图中一个阈值，得到一个点。现在我们需要一个独立于阈值的评价指标来衡量这个医生的医术如何，也就是遍历所有的阈值，得到ROC曲线。还是一开始的那幅图，假设如下就是某个医生的诊断统计图，直线代表阈值。我们遍历所有的阈值，能够在ROC平面上得到如下的ROC曲线。


![788753-20161121105504034-991875038.png | center | 533x488](https://cdn.nlark.com/lark/0/2018/png/18245/1544081457190-0679bd5e-3c54-449a-9e5f-c59e2cb54182.png "")

AUC值为ROC曲线所覆盖的区域面积，显然，AUC越大，分类器分类效果越好。
AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。
### 3.K-S图


